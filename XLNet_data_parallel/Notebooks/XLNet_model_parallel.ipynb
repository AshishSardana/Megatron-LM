{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM/XLNet_data_parallel\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/megatron/Megatron-LM/XLNet_data_parallel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 2 and model-parallel size: 2 \n",
      " > using dynamic loss scaling\n",
      "> initializing model parallel with size 2\n",
      "Pretrain XLNet model\n",
      "arguments:\n",
      "  pretrained_bert .............. False\n",
      "  attention_dropout ............ 0.1\n",
      "  num_attention_heads .......... 16\n",
      "  hidden_size .................. 1024\n",
      "  intermediate_size ............ None\n",
      "  num_layers ................... 24\n",
      "  layernorm_epsilon ............ 1e-05\n",
      "  hidden_dropout ............... 0.1\n",
      "  max_position_embeddings ...... 512\n",
      "  vocab_size ................... 30522\n",
      "  deep_init .................... False\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  fp16 ......................... True\n",
      "  fp32_embedding ............... True\n",
      "  fp32_layernorm ............... True\n",
      "  fp32_tokentypes .............. False\n",
      "  fp32_allreduce ............... False\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  batch_size ................... 4\n",
      "  weight_decay ................. 0.01\n",
      "  checkpoint_activations ....... False\n",
      "  checkpoint_num_layers ........ 1\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 1000000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  tensorboard_dir .............. None\n",
      "  seed ......................... 1234\n",
      "  reset_position_ids ........... False\n",
      "  reset_attention_mask ......... False\n",
      "  eod_mask_loss ................ False\n",
      "  lr_decay_iters ............... 990000\n",
      "  lr_decay_style ............... linear\n",
      "  lr ........................... 0.0001\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ False\n",
      "  use_checkpoint_lr_scheduler .. False\n",
      "  save ......................... checkpoints/bert_345m_mp2\n",
      "  save_interval ................ 5000\n",
      "  no_save_optim ................ False\n",
      "  no_save_rng .................. False\n",
      "  load ......................... checkpoints/bert_345m_mp2\n",
      "  no_load_optim ................ False\n",
      "  no_load_rng .................. False\n",
      "  finetune ..................... False\n",
      "  resume_dataloader ............ True\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... 0\n",
      "  adlr_autoresume .............. False\n",
      "  adlr_autoresume_interval ..... 1000\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... False\n",
      "  strict_lambada ............... False\n",
      "  eval_hf ...................... False\n",
      "  load_openai .................. False\n",
      "  temperature .................. 1.0\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  out_seq_length ............... 1024\n",
      "  sample_input_file ............ \n",
      "  sample_output_file ........... \n",
      "  num_samples .................. 0\n",
      "  genfile ...................... None\n",
      "  recompute .................... False\n",
      "  model_parallel_size .......... 2\n",
      "  shuffle ...................... False\n",
      "  train_data ................... ['wikipedia']\n",
      "  use_npy_data_loader .......... False\n",
      "  train_data_path .............. \n",
      "  val_data_path ................ \n",
      "  test_data_path ............... \n",
      "  input_data_sizes_file ........ sizes.txt\n",
      "  delim ........................ ,\n",
      "  text_key ..................... sentence\n",
      "  eval_text_key ................ None\n",
      "  valid_data ................... None\n",
      "  split ........................ 949,50,1\n",
      "  test_data .................... None\n",
      "  lazy_loader .................. True\n",
      "  loose_json ................... False\n",
      "  presplit_sentences ........... True\n",
      "  num_workers .................. 2\n",
      "  tokenizer_model_type ......... bert-base-uncased\n",
      "  tokenizer_path ............... tokenizer.model\n",
      "  tokenizer_type ............... XLNetWordPieceTokenizer\n",
      "  cache_dir .................... cache\n",
      "  use_tfrecords ................ False\n",
      "  seq_length ................... 512\n",
      "  max_preds_per_seq ............ 80\n",
      "  reuse_len .................... 256\n",
      "  perm_size .................... 256\n",
      "  bi_data ...................... False\n",
      "  mask_alpha ................... 6\n",
      "  mask_beta .................... 1\n",
      "  num_predict .................. 85\n",
      "  mem_len ...................... 384\n",
      "  num_epoch .................... 100\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 2\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "configuring data\n",
      "loading XLnetWordPieceTokenizer ( bert-base-uncased ) from cache_dir  cache\n",
      "---data_utils/wordpiece.py/BertTokenizer/__init__.py---\n",
      "------\n",
      "loaded bert-base-uncased\n",
      "initialize XLNetDataset\n",
      "ds_len:  34\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7f050b9f3358>\n",
      "ds_len:  1\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7f050b9f3358>\n",
      "ds_len:  1\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7f050b9f3358>\n",
      "> padded vocab (size: 30522) with 198 dummy tokens (new size: 30720)\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/pretrain_xlnet_model_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
