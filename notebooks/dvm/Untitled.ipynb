{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM-working\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/megatron/Megatron-LM-working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 2 and model-parallel size: 2 \n",
      " > using dynamic loss scaling\n",
      "1 range(0, 2)\n",
      "> initializing model parallel with size 2\n",
      "0 range(0, 2)\n",
      "Pretrain BERT model\n",
      "arguments:\n",
      "  pretrained_bert .............. False\n",
      "  attention_dropout ............ 0.1\n",
      "  num_attention_heads .......... 16\n",
      "  hidden_size .................. 1024\n",
      "  intermediate_size ............ None\n",
      "  num_layers ................... 24\n",
      "  layernorm_epsilon ............ 1e-05\n",
      "  hidden_dropout ............... 0.1\n",
      "  max_position_embeddings ...... 512\n",
      "  vocab_size ................... 30522\n",
      "  deep_init .................... False\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  fp16 ......................... True\n",
      "  fp32_embedding ............... True\n",
      "  fp32_layernorm ............... True\n",
      "  fp32_tokentypes .............. False\n",
      "  fp32_allreduce ............... False\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  batch_size ................... 4\n",
      "  weight_decay ................. 0.01\n",
      "  checkpoint_activations ....... False\n",
      "  checkpoint_num_layers ........ 1\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 1000000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  tensorboard_dir .............. None\n",
      "  seed ......................... 1234\n",
      "  reset_position_ids ........... False\n",
      "  reset_attention_mask ......... False\n",
      "  eod_mask_loss ................ False\n",
      "  lr_decay_iters ............... 990000\n",
      "  lr_decay_style ............... linear\n",
      "  lr ........................... 0.0001\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ False\n",
      "  use_checkpoint_lr_scheduler .. False\n",
      "  save ......................... checkpoints/bert_345m_mp2\n",
      "  save_interval ................ 5000\n",
      "  no_save_optim ................ False\n",
      "  no_save_rng .................. False\n",
      "  load ......................... checkpoints/bert_345m_mp2\n",
      "  no_load_optim ................ False\n",
      "  no_load_rng .................. False\n",
      "  finetune ..................... False\n",
      "  resume_dataloader ............ True\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... 0\n",
      "  adlr_autoresume .............. False\n",
      "  adlr_autoresume_interval ..... 1000\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... False\n",
      "  strict_lambada ............... False\n",
      "  eval_hf ...................... False\n",
      "  load_openai .................. False\n",
      "  temperature .................. 1.0\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  out_seq_length ............... 1024\n",
      "  sample_input_file ............ \n",
      "  sample_output_file ........... \n",
      "  num_samples .................. 0\n",
      "  genfile ...................... None\n",
      "  recompute .................... False\n",
      "  model_parallel_size .......... 2\n",
      "  shuffle ...................... False\n",
      "  train_data ................... ['wikipedia']\n",
      "  use_npy_data_loader .......... False\n",
      "  train_data_path .............. \n",
      "  val_data_path ................ \n",
      "  test_data_path ............... \n",
      "  input_data_sizes_file ........ sizes.txt\n",
      "  delim ........................ ,\n",
      "  text_key ..................... sentence\n",
      "  eval_text_key ................ None\n",
      "  valid_data ................... None\n",
      "  split ........................ 949,50,1\n",
      "  test_data .................... None\n",
      "  lazy_loader .................. True\n",
      "  loose_json ................... False\n",
      "  presplit_sentences ........... True\n",
      "  num_workers .................. 2\n",
      "  tokenizer_model_type ......... bert-large-uncased\n",
      "  tokenizer_path ............... tokenizer.model\n",
      "  tokenizer_type ............... BertWordPieceTokenizer\n",
      "  cache_dir .................... cache\n",
      "  use_tfrecords ................ False\n",
      "  seq_length ................... 512\n",
      "  max_preds_per_seq ............ 80\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 2\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "configuring data\n",
      "loading BertWordPieceTokenizer ( bert-large-uncased ) from cache_dir  cache\n",
      "Model name 'bert-large-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt' was a path or url but couldn't find any file associated to this path or url.\n",
      "loaded bert-large-uncased\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_bert.py\", line 649, in <module>\n",
      "    main()\n",
      "  File \"pretrain_bert.py\", line 591, in main\n",
      "    args.tokenizer_num_type_tokens = get_train_val_test_data(args)\n",
      "  File \"pretrain_bert.py\", line 522, in get_train_val_test_data\n",
      "    (train_data, val_data, test_data), tokenizer = data_config.apply(args)\n",
      "  File \"/workspace/megatron/Megatron-LM-working/configure_data.py\", line 34, in apply\n",
      "    return make_loaders(args)\n",
      "  File \"/workspace/megatron/Megatron-LM-working/configure_data.py\", line 171, in make_loaders\n",
      "    train, tokenizer = data_utils.make_dataset(**data_set_args)\n",
      "  File \"/workspace/megatron/Megatron-LM-working/data_utils/__init__.py\", line 112, in make_dataset\n",
      "    pad_token, character_converage, **kwargs)\n",
      "  File \"/workspace/megatron/Megatron-LM-working/data_utils/tokenization.py\", line 39, in make_tokenizer\n",
      "    return BertWordPieceTokenizer(model_type, **kwargs)\n",
      "  File \"/workspace/megatron/Megatron-LM-working/data_utils/tokenization.py\", line 703, in __init__\n",
      "    self.text_tokenizer.max_len = int(1e12)\n",
      "AttributeError: 'NoneType' object has no attribute 'max_len'\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_bert.py\", line 649, in <module>\n",
      "    main()\n",
      "  File \"pretrain_bert.py\", line 591, in main\n",
      "    args.tokenizer_num_type_tokens = get_train_val_test_data(args)\n",
      "  File \"pretrain_bert.py\", line 542, in get_train_val_test_data\n",
      "    group=mpu.get_model_parallel_group())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 751, in broadcast\n",
      "    work = group.broadcast([tensor], opts)\n",
      "RuntimeError: Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 235, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 231, in main\n",
      "    cmd=process.args)\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_bert.py', '--local_rank=0', '--model-parallel-size', '2', '--num-layers', '24', '--hidden-size', '1024', '--num-attention-heads', '16', '--batch-size', '4', '--seq-length', '512', '--max-preds-per-seq', '80', '--max-position-embeddings', '512', '--train-iters', '1000000', '--save', 'checkpoints/bert_345m_mp2', '--load', 'checkpoints/bert_345m_mp2', '--resume-dataloader', '--train-data', 'wikipedia', '--lazy-loader', '--tokenizer-type', 'BertWordPieceTokenizer', '--tokenizer-model-type', 'bert-large-uncased', '--presplit-sentences', '--cache-dir', 'cache', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '0.0001', '--lr-decay-style', 'linear', '--lr-decay-iters', '990000', '--weight-decay', '1e-2', '--clip-grad', '1.0', '--warmup', '.01', '--fp16', '--fp32-layernorm', '--fp32-embedding']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/pretrain_bert_model_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
