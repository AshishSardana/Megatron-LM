{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 8 and model-parallel size: 2 \n",
      " > using dynamic loss scaling\n",
      "> initializing model parallel with size 2\n",
      "Pretrain GPT2 model\n",
      "arguments:\n",
      "  pretrained_bert .............. False\n",
      "  attention_dropout ............ 0.1\n",
      "  num_attention_heads .......... 16\n",
      "  hidden_size .................. 1024\n",
      "  intermediate_size ............ None\n",
      "  num_layers ................... 24\n",
      "  layernorm_epsilon ............ 1e-05\n",
      "  hidden_dropout ............... 0.1\n",
      "  max_position_embeddings ...... 1024\n",
      "  vocab_size ................... 30522\n",
      "  deep_init .................... False\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  fp16 ......................... True\n",
      "  fp32_embedding ............... False\n",
      "  fp32_layernorm ............... False\n",
      "  fp32_tokentypes .............. False\n",
      "  fp32_allreduce ............... False\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  batch_size ................... 8\n",
      "  weight_decay ................. 0.01\n",
      "  checkpoint_activations ....... True\n",
      "  checkpoint_num_layers ........ 1\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 320000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  tensorboard_dir .............. None\n",
      "  seed ......................... 1234\n",
      "  reset_position_ids ........... False\n",
      "  reset_attention_mask ......... False\n",
      "  eod_mask_loss ................ False\n",
      "  lr_decay_iters ............... None\n",
      "  lr_decay_style ............... cosine\n",
      "  lr ........................... 0.00015\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ False\n",
      "  use_checkpoint_lr_scheduler .. False\n",
      "  save ......................... checkpoints/gpt2_345m_mp2\n",
      "  save_interval ................ 5000\n",
      "  no_save_optim ................ False\n",
      "  no_save_rng .................. False\n",
      "  load ......................... checkpoints/gpt2_345m_mp2\n",
      "  no_load_optim ................ False\n",
      "  no_load_rng .................. False\n",
      "  finetune ..................... False\n",
      "  resume_dataloader ............ True\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... 0\n",
      "  adlr_autoresume .............. False\n",
      "  adlr_autoresume_interval ..... 1000\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... False\n",
      "  strict_lambada ............... False\n",
      "  eval_hf ...................... False\n",
      "  load_openai .................. False\n",
      "  temperature .................. 1.0\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  out_seq_length ............... 1024\n",
      "  sample_input_file ............ \n",
      "  sample_output_file ........... \n",
      "  num_samples .................. 0\n",
      "  genfile ...................... None\n",
      "  recompute .................... False\n",
      "  model_parallel_size .......... 2\n",
      "  shuffle ...................... False\n",
      "  train_data ................... ['wikipedia']\n",
      "  use_npy_data_loader .......... False\n",
      "  train_data_path .............. \n",
      "  val_data_path ................ \n",
      "  test_data_path ............... \n",
      "  input_data_sizes_file ........ sizes.txt\n",
      "  delim ........................ ,\n",
      "  text_key ..................... sentence\n",
      "  eval_text_key ................ None\n",
      "  valid_data ................... None\n",
      "  split ........................ 949,50,1\n",
      "  test_data .................... None\n",
      "  lazy_loader .................. True\n",
      "  loose_json ................... False\n",
      "  presplit_sentences ........... False\n",
      "  num_workers .................. 2\n",
      "  tokenizer_model_type ......... bert-large-uncased\n",
      "  tokenizer_path ............... tokenizer.model\n",
      "  tokenizer_type ............... GPT2BPETokenizer\n",
      "  cache_dir .................... cache\n",
      "  use_tfrecords ................ False\n",
      "  seq_length ................... 1024\n",
      "  max_preds_per_seq ............ None\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 8\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "configuring data\n",
      "----debugger before tokenizer----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7fe7a2a5da90>\n",
      "----debugger before tokenizer----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7ff222ee49e8>\n",
      "----debugger before tokenizer----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7f0ef8de0a58>\n",
      "----debugger before tokenizer----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7f65662b3be0>\n",
      "----debugger before split----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7f65662b3be0>\n",
      "----debugger after split----\n",
      "[<data_utils.datasets.SplitDataset object at 0x7f655c6319b0>, <data_utils.datasets.SplitDataset object at 0x7f655c631978>, <data_utils.datasets.SplitDataset object at 0x7f655c6319e8>]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[10488 25593  6786 43927 72740 12741 32116 22244 45538 41388 55800  6690\n",
      " 37023 78354 17835 30101 90782  1974 61654 50821 48803 16710 34147  1000\n",
      "  6826 47260  6357  1842   116   798    91  3209 40118  4561]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f655c6319b0>\n",
      "---acc_lens_debugger---\n",
      "[10488, 36081, 42867, 86794, 159534, 172275, 204391, 226635, 272173, 313561, 369361, 376051, 413074, 491428, 509263, 539364, 630146, 632120, 693774, 744595, 793398, 810108, 844255, 845255, 852081, 899341, 905698, 907540, 907656, 908454, 908545, 911754, 951872, 956433]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[2070]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f655c631978>\n",
      "---acc_lens_debugger---\n",
      "[2070]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[8618]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f655c6319e8>\n",
      "---acc_lens_debugger---\n",
      "[8618]\n",
      "----debugger before split----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7ff222ee49e8>\n",
      "----debugger after split----\n",
      "[<data_utils.datasets.SplitDataset object at 0x7ff219261780>, <data_utils.datasets.SplitDataset object at 0x7ff219261748>, <data_utils.datasets.SplitDataset object at 0x7ff2192617b8>]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[10488 25593  6786 43927 72740 12741 32116 22244 45538 41388 55800  6690\n",
      " 37023 78354 17835 30101 90782  1974 61654 50821 48803 16710 34147  1000\n",
      "  6826 47260  6357  1842   116   798    91  3209 40118  4561]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7ff219261780>\n",
      "---acc_lens_debugger---\n",
      "[10488, 36081, 42867, 86794, 159534, 172275, 204391, 226635, 272173, 313561, 369361, 376051, 413074, 491428, 509263, 539364, 630146, 632120, 693774, 744595, 793398, 810108, 844255, 845255, 852081, 899341, 905698, 907540, 907656, 908454, 908545, 911754, 951872, 956433]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[2070]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7ff219261748>\n",
      "---acc_lens_debugger---\n",
      "[2070]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[8618]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7ff2192617b8>\n",
      "---acc_lens_debugger---\n",
      "[8618]\n",
      "----debugger before split----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7fe7a2a5da90>\n",
      "----debugger after split----\n",
      "[<data_utils.datasets.SplitDataset object at 0x7fe78fdec828>, <data_utils.datasets.SplitDataset object at 0x7fe78fdec7f0>, <data_utils.datasets.SplitDataset object at 0x7fe78fdec860>]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[10488 25593  6786 43927 72740 12741 32116 22244 45538 41388 55800  6690\n",
      " 37023 78354 17835 30101 90782  1974 61654 50821 48803 16710 34147  1000\n",
      "  6826 47260  6357  1842   116   798    91  3209 40118  4561]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7fe78fdec828>\n",
      "---acc_lens_debugger---\n",
      "[10488, 36081, 42867, 86794, 159534, 172275, 204391, 226635, 272173, 313561, 369361, 376051, 413074, 491428, 509263, 539364, 630146, 632120, 693774, 744595, 793398, 810108, 844255, 845255, 852081, 899341, 905698, 907540, 907656, 908454, 908545, 911754, 951872, 956433]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[2070]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7fe78fdec7f0>\n",
      "---acc_lens_debugger---\n",
      "[2070]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[8618]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7fe78fdec860>\n",
      "---acc_lens_debugger---\n",
      "[8618]\n",
      "> padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)\n",
      "> found end-of-document token: 50256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building GPT2 model ...\n",
      " > number of parameters on model parallel rank 1: 178100224\n",
      " > number of parameters on model parallel rank 0: 178100224\n",
      "----debugger before split----\n",
      "<data_utils.lazy_loader.lazy_array_loader object at 0x7f0ef8de0a58>\n",
      "----debugger after split----\n",
      "[<data_utils.datasets.SplitDataset object at 0x7f0edd12b7f0>, <data_utils.datasets.SplitDataset object at 0x7f0edd12b7b8>, <data_utils.datasets.SplitDataset object at 0x7f0edd12b828>]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[10488 25593  6786 43927 72740 12741 32116 22244 45538 41388 55800  6690\n",
      " 37023 78354 17835 30101 90782  1974 61654 50821 48803 16710 34147  1000\n",
      "  6826 47260  6357  1842   116   798    91  3209 40118  4561]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f0edd12b7f0>\n",
      "---acc_lens_debugger---\n",
      "[10488, 36081, 42867, 86794, 159534, 172275, 204391, 226635, 272173, 313561, 369361, 376051, 413074, 491428, 509263, 539364, 630146, 632120, 693774, 744595, 793398, 810108, 844255, 845255, 852081, 899341, 905698, 907540, 907656, 908454, 908545, 911754, 951872, 956433]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[2070]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f0edd12b7b8>\n",
      "---acc_lens_debugger---\n",
      "[2070]\n",
      "----weighted is true----\n",
      "----lazy is true----\n",
      "----init_weighting_debugger-lens----\n",
      "<class 'numpy.ndarray'>\n",
      "[8618]\n",
      "----init_weighting_debugger-dataset----\n",
      "<data_utils.datasets.SplitDataset object at 0x7f0edd12b828>\n",
      "---acc_lens_debugger---\n",
      "[8618]\n",
      "learning rate decaying cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_345m_mp2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      " iteration      100/  320000 | elapsed time per iteration (ms): 787.9 | learning rate 3.984E-06 | lm loss 9.764173E+00 | loss scale 262144.0 |\n",
      "after 100 iterations memory (MB) | allocated: 3431.068359375 | max allocated: 6221.71923828125 | cached: 6464.0 | max cached: 6464.0\n",
      "time (ms) | forward: 239.03 | backward: 540.71 | allreduce: 23.88 | optimizer: 7.91 | batch generator: 10.91 | data loader: 9.99\n",
      " iteration      200/  320000 | elapsed time per iteration (ms): 768.0 | learning rate 8.672E-06 | lm loss 8.309991E+00 | loss scale 262144.0 |\n",
      "time (ms) | forward: 219.39 | backward: 539.15 | allreduce: 22.36 | optimizer: 9.20 | batch generator: 1.01 | data loader: 0.12\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt2.py\", line 680, in <module>\n",
      "    \"__main__\", mod_spec)\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    main()\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 235, in <module>\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    timers, args, writer)\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    timers, args, writer)\n",
      "    timers, args, writer)\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "    timers, args, writer)\n",
      "    args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 228, in main\n",
      "    timers, args, writer)\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "    args, timers)\n",
      "    args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "    main()\n",
      "  File \"pretrain_gpt2.py\", line 655, in main\n",
      "    args, timers)\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "    timers, args, writer)\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "    process.wait()\n",
      "  File \"/opt/conda/lib/python3.6/subprocess.py\", line 1477, in wait\n",
      "    timers, args, writer)\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "    optimizer.update_master_grads()\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    optimizer.update_master_grads()\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    args, timers)\n",
      "    args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    timers, args, writer)\n",
      "  File \"pretrain_gpt2.py\", line 373, in train\n",
      "    self._check_overflow()\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "    args, timers)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"pretrain_gpt2.py\", line 333, in train_step\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "  File \"/opt/conda/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    cpu_sum = float(x.float().sum())\n",
      "    cpu_sum = float(x.float().sum())\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "KeyboardInterrupt\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "KeyboardInterrupt\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    lm_loss_reduced = backward_step(optimizer, model, lm_loss, args, timers)\n",
      "    cpu_sum = float(x.float().sum())\n",
      "  File \"pretrain_gpt2.py\", line 310, in backward_step\n",
      "KeyboardInterrupt\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    cpu_sum = float(x.float().sum())\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "KeyboardInterrupt\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    cpu_sum = float(x.float().sum())\n",
      "KeyboardInterrupt\n",
      "    optimizer.update_master_grads()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 564, in update_master_grads\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "    cpu_sum = float(x.float().sum())\n",
      "    cpu_sum = float(x.float().sum())\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "    self._check_overflow()\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/fp16.py\", line 286, in _check_overflow\n",
      "    self.overflow = self.loss_scaler.has_overflow(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 115, in has_overflow\n",
      "    overflow = self.has_overflow_serial(params)\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 109, in has_overflow_serial\n",
      "    if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
      "  File \"/workspace/megatron/Megatron-LM/fp16/loss_scaler.py\", line 132, in _has_inf_or_nan\n",
      "    cpu_sum = float(x.float().sum())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/pretrain_gpt2_model_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
