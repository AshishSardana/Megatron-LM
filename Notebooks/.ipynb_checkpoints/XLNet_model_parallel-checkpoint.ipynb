{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM/XLNet_data_parallel\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/megatron/Megatron-LM/XLNet_data_parallel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 2 and model-parallel size: 2 \n",
      " > using dynamic loss scaling\n",
      "> initializing model parallel with size 2\n",
      "Pretrain XLNet model\n",
      "arguments:\n",
      "  pretrained_bert .............. False\n",
      "  attention_dropout ............ 0.1\n",
      "  num_attention_heads .......... 16\n",
      "  hidden_size .................. 1024\n",
      "  intermediate_size ............ None\n",
      "  num_layers ................... 24\n",
      "  layernorm_epsilon ............ 1e-05\n",
      "  hidden_dropout ............... 0.1\n",
      "  max_position_embeddings ...... 512\n",
      "  vocab_size ................... 30522\n",
      "  deep_init .................... False\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  fp16 ......................... True\n",
      "  fp32_embedding ............... True\n",
      "  fp32_layernorm ............... True\n",
      "  fp32_tokentypes .............. False\n",
      "  fp32_allreduce ............... False\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  batch_size ................... 4\n",
      "  weight_decay ................. 0.01\n",
      "  checkpoint_activations ....... False\n",
      "  checkpoint_num_layers ........ 1\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 1000000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  tensorboard_dir .............. None\n",
      "  seed ......................... 1234\n",
      "  reset_position_ids ........... False\n",
      "  reset_attention_mask ......... False\n",
      "  eod_mask_loss ................ False\n",
      "  lr_decay_iters ............... 990000\n",
      "  lr_decay_style ............... linear\n",
      "  lr ........................... 0.0001\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ False\n",
      "  use_checkpoint_lr_scheduler .. False\n",
      "  save ......................... checkpoints/bert_345m_mp2\n",
      "  save_interval ................ 5000\n",
      "  no_save_optim ................ False\n",
      "  no_save_rng .................. False\n",
      "  load ......................... checkpoints/bert_345m_mp2\n",
      "  no_load_optim ................ False\n",
      "  no_load_rng .................. False\n",
      "  finetune ..................... False\n",
      "  resume_dataloader ............ True\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... 0\n",
      "  adlr_autoresume .............. False\n",
      "  adlr_autoresume_interval ..... 1000\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... False\n",
      "  strict_lambada ............... False\n",
      "  eval_hf ...................... False\n",
      "  load_openai .................. False\n",
      "  temperature .................. 1.0\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  out_seq_length ............... 1024\n",
      "  sample_input_file ............ \n",
      "  sample_output_file ........... \n",
      "  num_samples .................. 0\n",
      "  genfile ...................... None\n",
      "  recompute .................... False\n",
      "  model_parallel_size .......... 2\n",
      "  shuffle ...................... False\n",
      "  train_data ................... ['wikipedia']\n",
      "  use_npy_data_loader .......... False\n",
      "  train_data_path .............. \n",
      "  val_data_path ................ \n",
      "  test_data_path ............... \n",
      "  input_data_sizes_file ........ sizes.txt\n",
      "  delim ........................ ,\n",
      "  text_key ..................... sentence\n",
      "  eval_text_key ................ None\n",
      "  valid_data ................... None\n",
      "  split ........................ 949,50,1\n",
      "  test_data .................... None\n",
      "  lazy_loader .................. True\n",
      "  loose_json ................... False\n",
      "  presplit_sentences ........... True\n",
      "  num_workers .................. 2\n",
      "  tokenizer_model_type ......... bert-base-uncased\n",
      "  tokenizer_path ............... tokenizer.model\n",
      "  tokenizer_type ............... XLNetWordPieceTokenizer\n",
      "  cache_dir .................... cache\n",
      "  use_tfrecords ................ False\n",
      "  seq_length ................... 512\n",
      "  max_preds_per_seq ............ 80\n",
      "  reuse_len .................... 256\n",
      "  perm_size .................... 256\n",
      "  bi_data ...................... False\n",
      "  mask_alpha ................... 6\n",
      "  mask_beta .................... 1\n",
      "  num_predict .................. 85\n",
      "  mem_len ...................... 384\n",
      "  num_epoch .................... 100\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 2\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "configuring data\n",
      "loading XLnetWordPieceTokenizer ( bert-base-uncased ) from cache_dir  cache\n",
      "---data_utils/wordpiece.py/BertTokenizer/__init__.py---\n",
      "------\n",
      "loaded bert-base-uncased\n",
      "initialize XLNetDataset\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fcd0506ae48>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fcd152471d0>\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fcd0506ae10>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fcd152471d0>\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fcd0506ae80>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fcd152471d0>\n",
      "> padded vocab (size: 30522) with 198 dummy tokens (new size: 30720)\n",
      "building XLNet model ...\n",
      " > number of parameters on model parallel rank 1: 169604098\n",
      " > number of parameters on model parallel rank 0: 169604098\n",
      "serialdata None\n",
      "learning rate decaying linear\n",
      "WARNING: could not find the metadata file checkpoints/bert_345m_mp2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "setting training data start iteration to 0\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  37\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  309\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "idx:  0\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  104\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  0\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  256\n",
      "i:  512\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1792\n",
      "i:  1280\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  1536\n",
      "i:  2560\n",
      "i:  1792\n",
      "i:  2816\n",
      "i:  2048\n",
      "i:  3072\n",
      "i:  2304\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  2560\n",
      "i:  3840\n",
      "i:  2816\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  3072\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  5120\n",
      "i:  5376\n",
      "idx:  1\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  30\n",
      "i:  5632\n",
      "i:  5888\n",
      "i:  6144\n",
      "i:  6400\n",
      "i:  0\n",
      "i:  6656\n",
      "i:  256\n",
      "i:  512\n",
      "i:  6912\n",
      "i:  768\n",
      "i:  7168\n",
      "i:  7424\n",
      "idx:  2\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  129\n",
      "i:  7680\n",
      "i:  7936\n",
      "i:  8192\n",
      "i:  8448\n",
      "i:  8704\n",
      "i:  8960\n",
      "i:  9216\n",
      "i:  9472\n",
      "i:  9728\n",
      "i:  9984\n",
      "i:  10240\n",
      "i:  10496\n",
      "i:  10752\n",
      "i:  11008\n",
      "i:  11264\n",
      "i:  11520\n",
      "i:  11776\n",
      "i:  12032\n",
      "i:  12288\n",
      "i:  12544\n",
      "i:  12800\n",
      "i:  13056\n",
      "i:  13312\n",
      "i:  0\n",
      "i:  13568\n",
      "i:  256\n",
      "i:  13824\n",
      "i:  14080\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  2816\n",
      "i:  3072\n",
      "idx:  4\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  71\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  0\n",
      "i:  3840\n",
      "i:  256\n",
      "i:  4096\n",
      "i:  512\n",
      "i:  4352\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  4608\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  4864\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  5120\n",
      "idx:  5\n",
      "ds_batch_type:  <class 'str'>\n",
      "i:  5376\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  118\n",
      "i:  5632\n",
      "i:  5888\n",
      "i:  6144\n",
      "i:  6400\n",
      "i:  6656\n",
      "i:  6912\n",
      "i:  7168\n",
      "i:  0\n",
      "i:  7424\n",
      "i:  256\n",
      "i:  512\n",
      "i:  7680\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  7936\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  3840\n",
      "idx:  3\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  4608\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  167\n",
      "i:  4864\n",
      "i:  5120\n",
      "i:  5376\n",
      "i:  5632\n",
      "i:  5888\n",
      "idx:  6\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  107\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  0\n",
      "i:  256\n",
      "i:  768\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  1536\n",
      "i:  2048\n",
      "i:  1792\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  2048\n",
      "i:  3328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  3584\n",
      "i:  3840\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  2816\n",
      "i:  3072\n",
      "idx:  7\n",
      "i:  3328\n",
      "i:  3584\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  158\n",
      "i:  3840\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  5120\n",
      "i:  5376\n",
      "i:  0\n",
      "i:  256\n",
      "i:  5632\n",
      "i:  512\n",
      "i:  5888\n",
      "i:  768\n",
      "i:  6144\n",
      "i:  1024\n",
      "i:  6400\n",
      "i:  1280\n",
      "i:  6656\n",
      "i:  1536\n",
      "i:  6912\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  7168\n",
      "i:  2304\n",
      "i:  7424\n",
      "i:  2560\n",
      "i:  7680\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  4096\n",
      "idx:  8\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  136\n",
      "i:  4352\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  5120\n",
      "i:  5376\n",
      "i:  5632\n",
      "i:  5888\n",
      "i:  6144\n",
      "i:  6400\n",
      "i:  6656\n",
      "serialdata [{'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], dtype=torch.int32), 'label': [tensor([0, 0, 0, 1])], 'target_mapping': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'target': tensor([[ 4166,  1011,  2577,  2985,  1998, 23408,  2413,  1012,  4787,  5477,\n",
      "          7352,  5693,  1010,  8283,  2015,  1010,  1998,  2005,  1010,  2029,\n",
      "          1010,  1999,  7352,  9283,  1012,  1996,  1012,  2002,  4202,  1012,\n",
      "          7994,  1010,  1998, 25600,  2007,  2140,  1010, 23289,  2140,  2032,\n",
      "          3038,  1010,  1000,  2064,  1999,  4709,  1000,  2200,  3000,  9013,\n",
      "          2638,  1010,  8040, 25600,  1000,  1025,  2009,  1012, 25600,  6118,\n",
      "          2005,  1010, 25600,  2003,  1011,  5105,  3538,  1999,  1012,  1012,\n",
      "          4787,  5477,  2818,  1000, 29395,  1000,  3115,  1010,  2015, 11569,\n",
      "          1012,  2002,  2716,  2067,  1010],\n",
      "        [ 4111,  3888,  1010,  1010,  5327,  2000,  3443,  2489,  1998,  2009,\n",
      "          1010,  2104,  1996, 18944,  1997,  1996,  1012,  2030,  3312,  4924,\n",
      "          1011,  2010,  1012, 12077,  1010,  8754,  1012,  1999,  1037,  3661,\n",
      "          2000,  2102,  1006,  1000,  1006,  2008,  4111,  1000,  2001,  1996,\n",
      "          2034,  2007,  1997,  1000,  3888,  8867,  1010,  1000,  1010,  1998,\n",
      "          2069,  2028,  5449,  1010,  2029, 11113, 13578,  1010,  1000,  1010,\n",
      "          2337,  8163,  1024, 20446,  1000,  3456,  3952,  2011,  1996,  1012,\n",
      "          2101,  1010,  8891,  5562,  1011,  4911,  1024,  2999,  1010,  1000,\n",
      "          2176,  3456,  2488,  2004,     0],\n",
      "        [ 1006,  1007,  1011,  1996,  1012,  1007, 14206, 18321,  1012,  1010,\n",
      "          2043,  2178,  1012,  4489,  2184,  3285,  1010,  1012,  2022,  2988,\n",
      "          3151,  2075,  1010,  3344,  2058,  1011,  6375,  9963,  5246,  1012,\n",
      "         13843,  1010,  1998,  1996,  2031,  3011, 18215, 20940,  1012,  1025,\n",
      "          1007,  6210,  1996, 14658,  1011,  1012,  2349,  1012,  2007,  1010,\n",
      "          2029,  1999,  1010,  1012,  2122,  2051,  1006,  1999,  1010,  2073,\n",
      "          9593,  1006, 13843,  1010, 13530,  6019,  2051,  3593,  1012,  2003,\n",
      "          1006,  2007,  1006,  2029,  1005,  1012,  1010,  2043,  2178, 11679,\n",
      "          3823,  4261,  1997,  1010,     0],\n",
      "        [13908,  2007,  1998,  1010,  1012,  1010,  2591, 13908,  1012, 19465,\n",
      "          2003,  1010, 21572,  5648,  6544, 20739, 22698,  1010,  2599,  1010,\n",
      "          1010,  1012, 25962, 15161,  2005,  1025,  2129,  1996, 10840,  1006,\n",
      "         16233,  2213,  5729,  1010,  2004,  9675,  1006, 22851,  2094,  2220,\n",
      "          1011,  1012,  2348,  3572,  9174,  1010,  2295,  2070,  2024,  4489,\n",
      "          1998,  5845,  1012,  1012,  1010,  1996,  2193,  1997,  2111, 11441,\n",
      "          1003,  1999,  2456,  2084,  1010,  2029,  2089,  2022,  1999,  1012,\n",
      "         19465,  1010,  2034, 22813,  1998,  4076,  1037,  1012,  2111,  2007,\n",
      "         19465,  2070,  2468,  2011,     0]]), 'target_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]), 'perm_mask': tensor([[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'input_k': tensor([[ 2019,  2137,  1999,  ...,  4662,   102,   101],\n",
      "        [ 4111,  3888,  4111,  ...,  2004,   102,   101],\n",
      "        [ 2248,  9593,  2051,  ...,  2144,   102,   101],\n",
      "        [19465, 19465,  2003,  ...,  3613,   102,   101]]), 'input_q': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, {'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], dtype=torch.int32), 'label': [tensor([0, 0, 0, 0])], 'target_mapping': tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'target': tensor([[ 2043,  1000,  2174,  1000,  2200,  3000,  9013,  2638,  1037,  1005,\n",
      "          1010,  2728,  1998, 19486, 10350,  1037,  2573,  1012, 25600,  6118,\n",
      "          6628,  1012,  2000,  1010,  1996,  1010,  1996,  1012,  1055,  2001,\n",
      "          1996,  1010,  1010,  1002,  2005,  2778,  1012, 25600, 11188,  1999,\n",
      "          1012, 23289,  2140,  2635,  2172,  1012,  2664,  2652,  1010,  2025,\n",
      "          1012,  2014,  1024,  1000,  2054, 25600,  1010,  2004,  3383,  1000,\n",
      "         29395,  1999,  2003,  4543, 25600,  2034,  4662,  1012,  2009,  2001,\n",
      "          4427,  2011, 15925,  1998,  2943,  1012,  5477,  1000, 29395, 25600,\n",
      "          4032,  1010,  2067,  1010,  2029],\n",
      "        [ 1000,  2001,  1024,  1012,  2921,  1012,  2060, 12960,  1000,  1037,\n",
      "         18312,  1000,  1000,  2005,  1996,  2773,  1000,  1012,  1010,  2337,\n",
      "          1010,  2043,  1996,  2866,  1998,  2152,  1010,  1012,  1997,  1010,\n",
      "          2164,  2028,  1997,  1010,  5125,  2307,  3711,  1000,  2932,  4900,\n",
      "          1996,  2338,  1006,  2003,  4989,  1012,  1996,  9996,  1012,  3557,\n",
      "          1012,  2028,  1000, 15109,  1010,  2048,  1010,  1998,  1010,  4439,\n",
      "          8163,  1010,  2048,  3456,   999,  1996,  1012,  2101,  8891,  1011,\n",
      "          1010,  2007,  1024,  2776,  1010,  2122,  2024,  4176,  1000,  1000,\n",
      "          3456,  1000,  2004,     0,     0],\n",
      "        [ 1011,  2126,  5871,  1997,  1012,  1996,  8019,  6075,  2029,  1012,\n",
      "          2051,  9537,  1000, 11396,  1006, 27937,  1996,  2023,  1012,  1996,\n",
      "          1006,  1000,  1010,  4094,  1010,  2000,  2169,  1012,  1996,  2248,\n",
      "          4879,  1996,  2087,  1012,  2023,  2051,  1012,  2023,  2051,  2003,\n",
      "         11396,  1006,  1000,  1006,  1000,  1047, 11937,  1007,  1012, 10697,\n",
      "          1997,  8206,  1012,  4998,  1010,  1012,  1999, 17666,  1997,  1012,\n",
      "          8206,  1010,  2025,  1006,  1007,  3115,  1005,  1012,  2009,  2051,\n",
      "          1037,  1007,  1012,  2003,  2036,  2005,  1006,  1007,  1012,  1012,\n",
      "          1996,  4261,  3823,  3988,  2012],\n",
      "        [ 1010,  2045,  2031,  1012,  2019,  2038,  2070,  1037,  9526,  1012,\n",
      "         16452,  1010, 19465,  2484,  1012,  1022,  1010,  2199,  2111,  1012,\n",
      "          1012,  1012,  2084,  1010,  2029,  2089,  2022,  1012, 19465,  2003,\n",
      "          3811, 11265,  1012,  2111,  2007, 19465,  1010,  2030,  6020,  1012,\n",
      "          6360,  1010,  2468,  2411,  1037,  1010, 25172,  2015,  1999,  1012,\n",
      "          2060,  2036,  1012, 19465,  1998,  1010,  2302,  2240,  2135,  2691,\n",
      "          1012,  2591,  1006,  1012,  3264,  8740,  2038, 10599,  1010,  2030,\n",
      "          4800,  2063, 18653,  1010,  1996,  4044,  1012, 13458,  1012,  1012,\n",
      "          1012,  1023,  2005,  1998,     0]]), 'target_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]), 'perm_mask': tensor([[[0., 1., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]]]), 'input_k': tensor([[ 2140,  2043,  2017,  ...,  4662,   102,   101],\n",
      "        [ 3800,  2046,  2028,  ...,  2004,   102,   101],\n",
      "        [ 1996, 20940,  2024,  ...,  2144,   102,   101],\n",
      "        [ 2003,  2053,  2124,  ..., 19465,   102,   101]]), 'input_q': tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, {'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], dtype=torch.int32), 'label': [tensor([0, 0, 0, 0])], 'target_mapping': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'target': tensor([[ 1012,  2140,  2152, 23889,  1012,  1010,  2071,  6040,  2035,  1024,\n",
      "          1000,  2054,  2071,  1045,  2525,  1000,  2275,  1010,  2004,  2613,\n",
      "          2147,  3000,  2051,  1010,  2426,  2068, 16245,  9044,  1025,  1000,\n",
      "          1999,  1000,  1010,  3000,  1010,  2728,  1998,  7250,  1000,  2517,\n",
      "          1012, 25600,  2015,  1000,  1996,  3538,  1010,  2029, 12731,  1000,\n",
      "          1000,  1998, 11791,  1000,  1012,  1012,  1010,  2394,  7109,  2019,\n",
      "          1999,  1011,  5105, 13533,  3538,  2011,  2137,  2577, 11045,  3007,\n",
      "          1012,  2018,  1000, 29395,  4814,  1007, 25600,  3538,  1996,  1010,\n",
      "          8283,  2015,  1010,  4662,     0],\n",
      "        [ 2862, 15354,  1012,  1996,  9996,  1011,  2398,  1012, 24187,  2350,\n",
      "          1010,  2012,  4286,  1996, 15109,  1000,  1012,  2043,  1010,  1012,\n",
      "          3557,  3200,  1000,  1000,  2590,  1997,  5020,  1000,  1012,  1996,\n",
      "         10037,  1012,  4586,  4176,  1012,  2833,  2003, 20228,  4765,  1998,\n",
      "          1997,  1010,  3888,  1997,  1012,  8891,  2038,  2010,  1012,  8891,\n",
      "          4372, 18908,  2000,  2040,  1012, 10369,  9453,  1012,  1996,  4176,\n",
      "          2147,  6211,  2007,  1037,  8054,  1000,  4111,  3032,  1012,  1996,\n",
      "          2012,  1010,  2100,  5499,  1010,  2107,  2004,  1012,  1999,  2760,\n",
      "          1010,  1012,  1996, 14695,  5490],\n",
      "        [13843,  1006,  1000,  1000,  1006,  1000,  1047,  1012, 10697,  1999,\n",
      "          4772,  4998,  1010,  2320,  1010,  1998,  2000,  2191,  5372,  2178,\n",
      "          1037,  1006, 23746,  1007,  1012,  2220,  5031, 12322,  1025,  1996,\n",
      "          9593,  1010,  2120,  1010,  2866,  1006, 27937,  1007,  1010, 13861,\n",
      "          1037,  1012,  3838,  1010,  2628,  4781,  1012,  1996,  2248,  1010,\n",
      "          2572,  1999,  2478,  1998,  1012,  1010,  1010,  1011,  1037,  2020,\n",
      "          2012,  1010,  1024,  1006, 13843,  1010,  2013,  1011,  1012,  2009,\n",
      "          1007,  1006,  1010,  2029,  2003,  2109,  1055,  1010,  1010, 13843,\n",
      "          2003, 11396,  4489,  1997,  1010],\n",
      "        [ 1012,  2009,  2003,  2591,  1010,  1998,  1012,  3265,  8030,  1010,\n",
      "         14443,  1012,  1006,  2004,  2094,  2060,  2111,  1012,  3264,  8740,\n",
      "         16774,  2594,  3305, 10976, 15756,  1000,  2458,  1012, 16774,  2594,\n",
      "          3086,  2000, 22239,  1012,  8740, 16774,  2594,  6927,  1010,  1011,\n",
      "          1010,  2107,  2004,  8327,  1010,  2500,  7384,  1010,  2027,  2079,\n",
      "          2433,  2000,  2084,  1010,  1999,  2625,  2336,  2348,  2089,  6576,\n",
      "          3754,  1012,  2336,  2007,  2152,  1011,  2000,  1010,  1010,  2691,\n",
      "         10266,  1010,  1996,  4044,  1010,  1998,  5876, 24558,  3468,  2116,\n",
      "          2015,  3633,  1012,  1012,  1012]]), 'target_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), 'perm_mask': tensor([[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'input_k': tensor([[ 3116, 23289,  2140,  ...,  4662,   102,   101],\n",
      "        [ 2862,  1997,  2190,  ...,  2350,   102,   101],\n",
      "        [ 5662,  2000, 13843,  ...,  2144,   102,   101],\n",
      "        [ 2348,  2411,  1999,  ..., 19465,   102,   101]]), 'input_q': tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, {'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], dtype=torch.int32), 'label': [tensor([0, 0, 0, 0])], 'target_mapping': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'target': tensor([[ 1996,  1010,  1000,  1999, 25600,  1005,  1000,  3788,  1000,  6991,\n",
      "         13910,  1000,  1000,  4942,  9956,  1037,  2003,  1012,  2023,  1037,\n",
      "         10955,  1010,  1012,  1996,  1055,  1000,  2791,  5014,  1011,  3347,\n",
      "          5132,  1012,  2930,  1010,  2007,  1010,  1000,  2197,  1000,  3788,\n",
      "          1000,  5132,  2930, 10735,  3195,  1006,  3822, 19383,  2006,  1011,\n",
      "          4257,  3321,  1010,  1010,  1011,  1010,  1010,  1010,  1010,  1010,\n",
      "          2659,  1998,  1010,  1010,  1038,  1999,  1012,  2009,  2001,  4427,\n",
      "          1997,  1996,  6641,  1012,  2000,  1000, 29395,  1010,  8283,  2015,\n",
      "          1010,  2029,  1010,  4662,     0],\n",
      "        [ 1000,  1996,  2011,  1012,  8891,  2370,  1012,  8891,  4372, 18908,\n",
      "          3888,  2083,  1012,  1996,  4176,  4872,  6082,  1012,  1037,  1010,\n",
      "          8891,  2622,  1010,  1998,  4176,  1012,  1007,  4703, 15488, 26492,\n",
      "          1996,  1012,  1000,  2563,  1010,  2040,  3544,  2000,  1012,  1996,\n",
      "          1012,  1010,  3888,  5854,  1012,  2348,  2645,  2061,  2012,  1010,\n",
      "          1012,  1010, 10423,  2776, 25938,  2096, 25367,  1996, 16360,  1012,\n",
      "          5490,  5657,  9453,  2099,  1996,  1998,  1037,  1012,  2174,  1010,\n",
      "          1996, 15084,  1010,  1012,  1999,  1996,  2214,  1012,  1012,  1010,\n",
      "          1012,  2000,  2027, 11473,  1998],\n",
      "        [ 5119,  1010,  1010, 13264,  1010,  1010,  2572,  2119,  1010,  1037,\n",
      "          1012,  1015,  1010,  1996,  1010,  2005,  1024,  1006, 12170,  2232,\n",
      "          1007,  1000,  2229,  5007,  1012,  2013,  3411, 15871,  1997, 12170,\n",
      "          1006,  1007,  1012,  1996,  1010, 20940,  1012,  3225,  2013, 21486,\n",
      "          1007,  1010,  2035,  1010,  1006,  1996, 20248,  3593,  2138,  1010,\n",
      "          1010,  2011,  2055,  2112,  2890,  1000,  1006, 14925, 18223,  3574,\n",
      "          1006, 22975,  2497,  1007,  1006,  1010,  2013,  1996,  3593,  1012,\n",
      "          2613,  2051,  1006,  2007,  1037,  4964, 16396,  1007,  2035,  1012,\n",
      "          1012,  1996,  4261,  3988,  1996],\n",
      "        [ 1010,  3921, 10047,  1010,  7384,  1012,  2087,  3036,  1010,  2348,\n",
      "          2023,  1012,  3080,  2336,  1998,  2227,  1012,  6387,  4102,  1011,\n",
      "          8740, 16774,  2022,  1998,  1012, 16014,  2015,  2107,  1010,  2089,\n",
      "          7461,  1996,  3737,  6171, 27364,  1010,  1010,  1012,  1996,  1010,\n",
      "          6215,  1997,  2015,  1012,  2013,  1010, 15911, 26651,  2791, 25549,\n",
      "          1012,  1998,  1010,  1010,  1012,  2336,  2007, 19465,  2024,  3745,\n",
      "          1010,  1998,  3497,  2500,  2062,  1010,  2030,  2011,  4678,  4800,\n",
      "          1010,  1010,  2024,  1998,  1012,  2116,  9165,  2031,  2042,  1012,\n",
      "          1021,  2005,  2004,  1010,  9504]]), 'target_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), 'perm_mask': tensor([[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'input_k': tensor([[ 2103,  1010,  4952,  ...,  4662,   102,   101],\n",
      "        [ 2101,  9188,  1996,  ...,  8163,   102,   101],\n",
      "        [ 9593,  8093,  2239,  ...,  2144,   102,   101],\n",
      "        [ 2007, 19465,  2024,  ..., 19465,   102,   101]]), 'input_q': tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}]\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_xlnet.py\", line 540, in <module>\n",
      "    main()\n",
      "  File \"pretrain_xlnet.py\", line 531, in main\n",
      "    timers, args, writer)#removing argument writer\n",
      "  File \"pretrain_xlnet.py\", line 322, in train\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 275, in train_step\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 207, in forward_step\n",
      "    padding_mask = get_batch(data_iterator, timers)\n",
      "  File \"pretrain_xlnet.py\", line 182, in get_batch\n",
      "    for key in data.keys():\n",
      "AttributeError: 'list' object has no attribute 'keys'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 235, in <module>\r\n",
      "    main()\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 231, in main\r\n",
      "    cmd=process.args)\r\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_xlnet.py', '--local_rank=0', '--model-parallel-size', '2', '--num-layers', '24', '--hidden-size', '1024', '--num-attention-heads', '16', '--batch-size', '4', '--seq-length', '512', '--max-preds-per-seq', '80', '--max-position-embeddings', '512', '--train-iters', '1000000', '--save', 'checkpoints/bert_345m_mp2', '--load', 'checkpoints/bert_345m_mp2', '--resume-dataloader', '--train-data', 'wikipedia', '--lazy-loader', '--tokenizer-type', 'XLNetWordPieceTokenizer', '--tokenizer-model-type', 'bert-base-uncased', '--presplit-sentences', '--cache-dir', 'cache', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '0.0001', '--lr-decay-style', 'linear', '--lr-decay-iters', '990000', '--weight-decay', '1e-2', '--clip-grad', '1.0', '--warmup', '.01', '--fp16', '--fp32-layernorm', '--fp32-embedding']' returned non-zero exit status 1.\r\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/pretrain_xlnet_model_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
