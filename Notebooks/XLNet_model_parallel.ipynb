{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/megatron/Megatron-LM/XLNet_data_parallel\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/megatron/Megatron-LM/XLNet_data_parallel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 2 and model-parallel size: 2 \n",
      " > using dynamic loss scaling\n",
      "> initializing model parallel with size 2\n",
      "Pretrain XLNet model\n",
      "arguments:\n",
      "  pretrained_bert .............. False\n",
      "  attention_dropout ............ 0.1\n",
      "  num_attention_heads .......... 16\n",
      "  hidden_size .................. 1024\n",
      "  intermediate_size ............ None\n",
      "  num_layers ................... 24\n",
      "  layernorm_epsilon ............ 1e-05\n",
      "  hidden_dropout ............... 0.1\n",
      "  max_position_embeddings ...... 512\n",
      "  vocab_size ................... 30522\n",
      "  deep_init .................... False\n",
      "  make_vocab_size_divisible_by . 128\n",
      "  fp16 ......................... True\n",
      "  fp32_embedding ............... True\n",
      "  fp32_layernorm ............... True\n",
      "  fp32_tokentypes .............. False\n",
      "  fp32_allreduce ............... False\n",
      "  hysteresis ................... 2\n",
      "  loss_scale ................... None\n",
      "  loss_scale_window ............ 1000\n",
      "  min_scale .................... 1\n",
      "  batch_size ................... 4\n",
      "  weight_decay ................. 0.01\n",
      "  checkpoint_activations ....... False\n",
      "  checkpoint_num_layers ........ 1\n",
      "  clip_grad .................... 1.0\n",
      "  train_iters .................. 1000000\n",
      "  log_interval ................. 100\n",
      "  exit_interval ................ None\n",
      "  tensorboard_dir .............. None\n",
      "  seed ......................... 1234\n",
      "  reset_position_ids ........... False\n",
      "  reset_attention_mask ......... False\n",
      "  eod_mask_loss ................ False\n",
      "  lr_decay_iters ............... 990000\n",
      "  lr_decay_style ............... linear\n",
      "  lr ........................... 0.0001\n",
      "  min_lr ....................... 0.0\n",
      "  warmup ....................... 0.01\n",
      "  override_lr_scheduler ........ False\n",
      "  use_checkpoint_lr_scheduler .. False\n",
      "  save ......................... checkpoints/bert_345m_mp2\n",
      "  save_interval ................ 5000\n",
      "  no_save_optim ................ False\n",
      "  no_save_rng .................. False\n",
      "  load ......................... checkpoints/bert_345m_mp2\n",
      "  no_load_optim ................ False\n",
      "  no_load_rng .................. False\n",
      "  finetune ..................... False\n",
      "  resume_dataloader ............ True\n",
      "  distributed_backend .......... nccl\n",
      "  DDP_impl ..................... local\n",
      "  local_rank ................... 0\n",
      "  adlr_autoresume .............. False\n",
      "  adlr_autoresume_interval ..... 1000\n",
      "  eval_batch_size .............. None\n",
      "  eval_iters ................... 100\n",
      "  eval_interval ................ 1000\n",
      "  eval_seq_length .............. None\n",
      "  eval_max_preds_per_seq ....... None\n",
      "  overlapping_eval ............. 32\n",
      "  cloze_eval ................... False\n",
      "  strict_lambada ............... False\n",
      "  eval_hf ...................... False\n",
      "  load_openai .................. False\n",
      "  temperature .................. 1.0\n",
      "  greedy ....................... False\n",
      "  top_p ........................ 0.0\n",
      "  top_k ........................ 0\n",
      "  out_seq_length ............... 1024\n",
      "  sample_input_file ............ \n",
      "  sample_output_file ........... \n",
      "  num_samples .................. 0\n",
      "  genfile ...................... None\n",
      "  recompute .................... False\n",
      "  model_parallel_size .......... 2\n",
      "  shuffle ...................... False\n",
      "  train_data ................... ['wikipedia']\n",
      "  use_npy_data_loader .......... False\n",
      "  train_data_path .............. \n",
      "  val_data_path ................ \n",
      "  test_data_path ............... \n",
      "  input_data_sizes_file ........ sizes.txt\n",
      "  delim ........................ ,\n",
      "  text_key ..................... sentence\n",
      "  eval_text_key ................ None\n",
      "  valid_data ................... None\n",
      "  split ........................ 949,50,1\n",
      "  test_data .................... None\n",
      "  lazy_loader .................. True\n",
      "  loose_json ................... False\n",
      "  presplit_sentences ........... True\n",
      "  num_workers .................. 2\n",
      "  tokenizer_model_type ......... bert-base-uncased\n",
      "  tokenizer_path ............... tokenizer.model\n",
      "  tokenizer_type ............... XLNetWordPieceTokenizer\n",
      "  cache_dir .................... cache\n",
      "  use_tfrecords ................ False\n",
      "  seq_length ................... 512\n",
      "  max_preds_per_seq ............ 80\n",
      "  reuse_len .................... 256\n",
      "  perm_size .................... 256\n",
      "  bi_data ...................... False\n",
      "  mask_alpha ................... 6\n",
      "  mask_beta .................... 1\n",
      "  num_predict .................. 85\n",
      "  mem_len ...................... 384\n",
      "  num_epoch .................... 100\n",
      "  cuda ......................... True\n",
      "  rank ......................... 0\n",
      "  world_size ................... 2\n",
      "  dynamic_loss_scale ........... True\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "configuring data\n",
      "loading XLnetWordPieceTokenizer ( bert-base-uncased ) from cache_dir  cache\n",
      "---data_utils/wordpiece.py/BertTokenizer/__init__.py---\n",
      "------\n",
      "loaded bert-base-uncased\n",
      "initialize XLNetDataset\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fc03051ee80>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fc03a6e9208>\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fc03051ee48>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fc03a6e9208>\n",
      "ds:  <data_utils.datasets.SplitDataset object at 0x7fc03051eeb8>\n",
      "#####-----data_utils/datasets.py/XLNetDataset>__init__>ln(597...)-----#####\n",
      "tokenizer:  <data_utils.tokenization.XLNetWordPieceTokenizer object at 0x7fc03a6e9208>\n",
      "> padded vocab (size: 30522) with 198 dummy tokens (new size: 30720)\n",
      "building XLNet model ...\n",
      " > number of parameters on model parallel rank 1: 169604098\n",
      " > number of parameters on model parallel rank 0: 169604098\n",
      "learning rate decaying linear\n",
      "WARNING: could not find the metadata file checkpoints/bert_345m_mp2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "setting training data start iteration to 0\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  37\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  309\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  104\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  0\n",
      "i:  1536\n",
      "i:  256\n",
      "i:  1792\n",
      "i:  512\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  768\n",
      "i:  2560\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  1536\n",
      "i:  3328\n",
      "i:  1792\n",
      "i:  3584\n",
      "i:  2048\n",
      "i:  3840\n",
      "i:  2304\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  2560\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  2816\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  30\n",
      "i:  3072\n",
      "i:  3328\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  0\n",
      "i:  4608\n",
      "i:  256\n",
      "i:  4864\n",
      "i:  512\n",
      "i:  768\n",
      "i:  5120\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  129\n",
      "i:  5376\n",
      "i:  5632\n",
      "i:  5888\n",
      "i:  6144\n",
      "i:  6400\n",
      "i:  6656\n",
      "i:  6912\n",
      "i:  7168\n",
      "i:  7424\n",
      "i:  7680\n",
      "i:  7936\n",
      "i:  8192\n",
      "i:  8448\n",
      "i:  8704\n",
      "i:  8960\n",
      "i:  9216\n",
      "i:  9472\n",
      "i:  9728\n",
      "i:  9984\n",
      "i:  10240\n",
      "i:  10496\n",
      "i:  10752\n",
      "i:  0\n",
      "i:  11008\n",
      "i:  11264\n",
      "i:  256\n",
      "i:  11520\n",
      "i:  512\n",
      "i:  11776\n",
      "i:  12032\n",
      "i:  768\n",
      "i:  12288\n",
      "i:  1024\n",
      "i:  12544\n",
      "i:  12800\n",
      "i:  1280\n",
      "i:  13056\n",
      "i:  13312\n",
      "i:  13568\n",
      "i:  1536\n",
      "i:  13824\n",
      "i:  14080\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  71\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  0\n",
      "i:  2560\n",
      "i:  256\n",
      "i:  512\n",
      "i:  2816\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  3072\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  3328\n",
      "i:  2048\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  118\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  4608\n",
      "i:  4864\n",
      "i:  0\n",
      "i:  5120\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  5376\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  5632\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  5888\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  6144\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  3328\n",
      "i:  6400\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  6656\n",
      "i:  4096\n",
      "i:  4352\n",
      "i:  6912\n",
      "i:  4608\n",
      "i:  7168\n",
      "i:  4864\n",
      "i:  5120\n",
      "i:  7424\n",
      "i:  5376\n",
      "i:  5632\n",
      "i:  7680\n",
      "i:  5888\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  107\n",
      "i:  7936\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  167\n",
      "i:  0\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  2048\n",
      "i:  2304\n",
      "i:  2560\n",
      "i:  2816\n",
      "i:  3072\n",
      "i:  3328\n",
      "i:  0\n",
      "i:  3584\n",
      "i:  3840\n",
      "i:  256\n",
      "ds_batch_type:  <class 'str'>\n",
      "lines type:  <class 'list'>\n",
      "len_lines:  158\n",
      "i:  512\n",
      "i:  768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  1024\n",
      "i:  1280\n",
      "i:  1536\n",
      "i:  1792\n",
      "i:  2048\n",
      "seg_id type is\n",
      "torch.int64\n",
      "label type is\n",
      "torch.int64\n",
      "target_mapping type is\n",
      "torch.int64\n",
      "target type is\n",
      "torch.int64\n",
      "target_mask type is\n",
      "torch.int64\n",
      "perm_mask type is\n",
      "torch.int64\n",
      "input_k type is\n",
      "torch.int64\n",
      "input_q type is\n",
      "torch.int64\n",
      "i:  0\n",
      "i:  2304\n",
      "i:  256\n",
      "i:  512\n",
      "i:  768\n",
      "i:  2560\n",
      "i:  2816\n",
      "this is data pll {'input_k': tensor([[ 2019,  2137,  1999,  ...,  4662,   102,   101],\n",
      "        [ 4111,  3888,  4111,  ...,  2004,   102,   101],\n",
      "        [ 2248,  9593,  2051,  ...,  2144,   102,   101],\n",
      "        [19465, 19465,  2003,  ...,  3613,   102,   101]], device='cuda:1'), 'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], device='cuda:1'), 'target': tensor([[ 4166,  1011,  2577,  2985,  1998, 23408,  2413,  1012,  4787,  5477,\n",
      "          7352,  5693,  1010,  8283,  2015,  1010,  1998,  2005,  1010,  2029,\n",
      "          1010,  1999,  7352,  9283,  1012,  1996,  1012,  2002,  4202,  1012,\n",
      "          7994,  1010,  1998, 25600,  2007,  2140,  1010, 23289,  2140,  2032,\n",
      "          3038,  1010,  1000,  2064,  1999,  4709,  1000,  2200,  3000,  9013,\n",
      "          2638,  1010,  8040, 25600,  1000,  1025,  2009,  1012, 25600,  6118,\n",
      "          2005,  1010, 25600,  2003,  1011,  5105,  3538,  1999,  1012,  1012,\n",
      "          4787,  5477,  2818,  1000, 29395,  1000,  3115,  1010,  2015, 11569,\n",
      "          1012,  2002,  2716,  2067,  1010],\n",
      "        [ 4111,  3888,  1010,  1010,  5327,  2000,  3443,  2489,  1998,  2009,\n",
      "          1010,  2104,  1996, 18944,  1997,  1996,  1012,  2030,  3312,  4924,\n",
      "          1011,  2010,  1012, 12077,  1010,  8754,  1012,  1999,  1037,  3661,\n",
      "          2000,  2102,  1006,  1000,  1006,  2008,  4111,  1000,  2001,  1996,\n",
      "          2034,  2007,  1997,  1000,  3888,  8867,  1010,  1000,  1010,  1998,\n",
      "          2069,  2028,  5449,  1010,  2029, 11113, 13578,  1010,  1000,  1010,\n",
      "          2337,  8163,  1024, 20446,  1000,  3456,  3952,  2011,  1996,  1012,\n",
      "          2101,  1010,  8891,  5562,  1011,  4911,  1024,  2999,  1010,  1000,\n",
      "          2176,  3456,  2488,  2004,     0],\n",
      "        [ 1006,  1007,  1011,  1996,  1012,  1007, 14206, 18321,  1012,  1010,\n",
      "          2043,  2178,  1012,  4489,  2184,  3285,  1010,  1012,  2022,  2988,\n",
      "          3151,  2075,  1010,  3344,  2058,  1011,  6375,  9963,  5246,  1012,\n",
      "         13843,  1010,  1998,  1996,  2031,  3011, 18215, 20940,  1012,  1025,\n",
      "          1007,  6210,  1996, 14658,  1011,  1012,  2349,  1012,  2007,  1010,\n",
      "          2029,  1999,  1010,  1012,  2122,  2051,  1006,  1999,  1010,  2073,\n",
      "          9593,  1006, 13843,  1010, 13530,  6019,  2051,  3593,  1012,  2003,\n",
      "          1006,  2007,  1006,  2029,  1005,  1012,  1010,  2043,  2178, 11679,\n",
      "          3823,  4261,  1997,  1010,     0],\n",
      "        [13908,  2007,  1998,  1010,  1012,  1010,  2591, 13908,  1012, 19465,\n",
      "          2003,  1010, 21572,  5648,  6544, 20739, 22698,  1010,  2599,  1010,\n",
      "          1010,  1012, 25962, 15161,  2005,  1025,  2129,  1996, 10840,  1006,\n",
      "         16233,  2213,  5729,  1010,  2004,  9675,  1006, 22851,  2094,  2220,\n",
      "          1011,  1012,  2348,  3572,  9174,  1010,  2295,  2070,  2024,  4489,\n",
      "          1998,  5845,  1012,  1012,  1010,  1996,  2193,  1997,  2111, 11441,\n",
      "          1003,  1999,  2456,  2084,  1010,  2029,  2089,  2022,  1999,  1012,\n",
      "         19465,  1010,  2034, 22813,  1998,  4076,  1037,  1012,  2111,  2007,\n",
      "         19465,  2070,  2468,  2011,     0]], device='cuda:1'), 'perm_mask': tensor([[[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 1, 1, 1],\n",
      "         [1, 0, 0,  ..., 1, 1, 1],\n",
      "         [1, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 1],\n",
      "         [0, 0, 0,  ..., 1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:1'), 'target_mapping': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:1'), 'input_q': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1'), 'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:1')}i:  1024\n",
      "\n",
      "input_k:  torch.int64\n",
      "seg_id:  torch.int32\n",
      "target:  torch.int64\n",
      "perm_mask:  torch.float32\n",
      "target_mapping:  torch.float32\n",
      "input_q:  torch.float32\n",
      "target_mask:  torch.float32\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_xlnet.py\", line 557, in <module>\n",
      "    main()\n",
      "  File \"pretrain_xlnet.py\", line 548, in main\n",
      "    timers, args, writer)#removing argument writer\n",
      "  File \"pretrain_xlnet.py\", line 339, in train\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 292, in train_step\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 224, in forward_step\n",
      "    padding_mask = get_batch(data_iterator, timers)\n",
      "ValueError: too many values to unpack (expected 6)\n",
      "this is data pll {'input_k': tensor([[ 2019,  2137,  1999,  ...,  4662,   102,   101],\n",
      "        [ 4111,  3888,  4111,  ...,  2004,   102,   101],\n",
      "        [ 2248,  9593,  2051,  ...,  2144,   102,   101],\n",
      "        [19465, 19465,  2003,  ...,  3613,   102,   101]], device='cuda:0'), 'seg_id': tensor([[0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2],\n",
      "        [0, 0, 0,  ..., 1, 1, 2]], device='cuda:0'), 'target': tensor([[ 4166,  1011,  2577,  2985,  1998, 23408,  2413,  1012,  4787,  5477,\n",
      "          7352,  5693,  1010,  8283,  2015,  1010,  1998,  2005,  1010,  2029,\n",
      "          1010,  1999,  7352,  9283,  1012,  1996,  1012,  2002,  4202,  1012,\n",
      "          7994,  1010,  1998, 25600,  2007,  2140,  1010, 23289,  2140,  2032,\n",
      "          3038,  1010,  1000,  2064,  1999,  4709,  1000,  2200,  3000,  9013,\n",
      "          2638,  1010,  8040, 25600,  1000,  1025,  2009,  1012, 25600,  6118,\n",
      "          2005,  1010, 25600,  2003,  1011,  5105,  3538,  1999,  1012,  1012,\n",
      "          4787,  5477,  2818,  1000, 29395,  1000,  3115,  1010,  2015, 11569,\n",
      "          1012,  2002,  2716,  2067,  1010],\n",
      "        [ 4111,  3888,  1010,  1010,  5327,  2000,  3443,  2489,  1998,  2009,\n",
      "          1010,  2104,  1996, 18944,  1997,  1996,  1012,  2030,  3312,  4924,\n",
      "          1011,  2010,  1012, 12077,  1010,  8754,  1012,  1999,  1037,  3661,\n",
      "          2000,  2102,  1006,  1000,  1006,  2008,  4111,  1000,  2001,  1996,\n",
      "          2034,  2007,  1997,  1000,  3888,  8867,  1010,  1000,  1010,  1998,\n",
      "          2069,  2028,  5449,  1010,  2029, 11113, 13578,  1010,  1000,  1010,\n",
      "          2337,  8163,  1024, 20446,  1000,  3456,  3952,  2011,  1996,  1012,\n",
      "          2101,  1010,  8891,  5562,  1011,  4911,  1024,  2999,  1010,  1000,\n",
      "          2176,  3456,  2488,  2004,     0],\n",
      "        [ 1006,  1007,  1011,  1996,  1012,  1007, 14206, 18321,  1012,  1010,\n",
      "          2043,  2178,  1012,  4489,  2184,  3285,  1010,  1012,  2022,  2988,\n",
      "          3151,  2075,  1010,  3344,  2058,  1011,  6375,  9963,  5246,  1012,\n",
      "         13843,  1010,  1998,  1996,  2031,  3011, 18215, 20940,  1012,  1025,\n",
      "          1007,  6210,  1996, 14658,  1011,  1012,  2349,  1012,  2007,  1010,\n",
      "          2029,  1999,  1010,  1012,  2122,  2051,  1006,  1999,  1010,  2073,\n",
      "          9593,  1006, 13843,  1010, 13530,  6019,  2051,  3593,  1012,  2003,\n",
      "          1006,  2007,  1006,  2029,  1005,  1012,  1010,  2043,  2178, 11679,\n",
      "          3823,  4261,  1997,  1010,     0],\n",
      "        [13908,  2007,  1998,  1010,  1012,  1010,  2591, 13908,  1012, 19465,\n",
      "          2003,  1010, 21572,  5648,  6544, 20739, 22698,  1010,  2599,  1010,\n",
      "          1010,  1012, 25962, 15161,  2005,  1025,  2129,  1996, 10840,  1006,\n",
      "         16233,  2213,  5729,  1010,  2004,  9675,  1006, 22851,  2094,  2220,\n",
      "          1011,  1012,  2348,  3572,  9174,  1010,  2295,  2070,  2024,  4489,\n",
      "          1998,  5845,  1012,  1012,  1010,  1996,  2193,  1997,  2111, 11441,\n",
      "          1003,  1999,  2456,  2084,  1010,  2029,  2089,  2022,  1999,  1012,\n",
      "         19465,  1010,  2034, 22813,  1998,  4076,  1037,  1012,  2111,  2007,\n",
      "         19465,  2070,  2468,  2011,     0]], device='cuda:0'), 'perm_mask': tensor([[[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 1, 1, 1],\n",
      "         [1, 0, 0,  ..., 1, 1, 1],\n",
      "         [1, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 1],\n",
      "         [0, 0, 0,  ..., 1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'), 'target_mapping': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'), 'input_q': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}\n",
      "input_k:  torch.int64\n",
      "seg_id:  torch.int32\n",
      "target:  torch.int64\n",
      "perm_mask:  torch.float32\n",
      "target_mapping:  torch.float32\n",
      "input_q:  torch.float32\n",
      "target_mask:  torch.float32\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_xlnet.py\", line 557, in <module>\n",
      "    main()\n",
      "  File \"pretrain_xlnet.py\", line 548, in main\n",
      "    timers, args, writer)#removing argument writer\n",
      "  File \"pretrain_xlnet.py\", line 339, in train\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 292, in train_step\n",
      "    args, timers)\n",
      "  File \"pretrain_xlnet.py\", line 224, in forward_step\n",
      "    padding_mask = get_batch(data_iterator, timers)\n",
      "ValueError: too many values to unpack (expected 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 235, in <module>\r\n",
      "    main()\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 231, in main\r\n",
      "    cmd=process.args)\r\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_xlnet.py', '--local_rank=0', '--model-parallel-size', '2', '--num-layers', '24', '--hidden-size', '1024', '--num-attention-heads', '16', '--batch-size', '4', '--seq-length', '512', '--max-preds-per-seq', '80', '--max-position-embeddings', '512', '--train-iters', '1000000', '--save', 'checkpoints/bert_345m_mp2', '--load', 'checkpoints/bert_345m_mp2', '--resume-dataloader', '--train-data', 'wikipedia', '--lazy-loader', '--tokenizer-type', 'XLNetWordPieceTokenizer', '--tokenizer-model-type', 'bert-base-uncased', '--presplit-sentences', '--cache-dir', 'cache', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '0.0001', '--lr-decay-style', 'linear', '--lr-decay-iters', '990000', '--weight-decay', '1e-2', '--clip-grad', '1.0', '--warmup', '.01', '--fp16', '--fp32-layernorm', '--fp32-embedding']' returned non-zero exit status 1.\r\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/pretrain_xlnet_model_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
